# -*- coding: utf-8 -*-
"""Linear_Regression_coefficients.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pj91Wv4oGEs9ykI8_DyDFYBLe8U_gIYY

You are given a generated regression dataset `D: (X, y)` along already split train and test subsets, `X_train, X_test, y_train, y_test`. You are also given the true underlying coefficients `coef`.

    Linear regression tasks
    -----------------------
    1. Build a linear regression model using sklearn.
    2. Compute the MSE on the train and test sets.
    3. Compare the true and learned coefficients.
    4. Plot the regression line for any two top coefficients.
    5. Describe what you have seen based on the experiements (minimum 150 words).

Resources:
1. [Linear Regression Model](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)
2. [Plotting](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html#sphx-glr-auto-examples-linear-model-plot-ols-py)
"""

import numpy as np
import pandas as pd
from sklearn.metrics import mean_squared_error
from sklearn.datasets import make_regression
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

#@title Data Generation (DO NOT MODIFY)
X, y, coef = make_regression(n_samples=500, n_features=15, n_informative=8, effective_rank=6.0, noise=3.0, coef=True, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

pd.DataFrame(X).corr()

#Correlation matrix
sns.heatmap(pd.DataFrame(X).corr(), annot=True)

## Note that this cell will take a long time to run as we are plotting all 15 features
Xdf = pd.DataFrame(X)
sns.pairplot(Xdf)

#Normal Distribution
Xdf = pd.DataFrame(X)
Xdf.hist()

#Linearity
for i in range(15):
  plt.scatter(X[:,i],y, label="X"+str(i))
plt.legend()

"""# Observations on Linear Regression assumptions

1. Independence: The correlation matrix shows very low to 0 correlation among the variables

2. Normality: All the variables are normally distributed

3. Linearity: All the variables are distributed linearly with respect to the dependent variable

Hence the above dataset maintains all the three assumptions of the linear regression

# **Linear Regression**
"""

#Training on 75% of the dataset and predicting on the remaining 25%
model = LinearRegression()

model.fit(X_train,y_train)
pred = model.predict(X_test)

#Prediction on the training dataset
pred_train = model.predict(X_train)

#MSE calculation on Training dataset
mse_train = mean_squared_error(y_train,pred_train)
print(mse_train)

x_axis=np.arange(1,126)
fig, ax = plt.subplots()
ax = sns.lineplot(x_axis,y_test)
ax_2 = sns.lineplot(x_axis, pred)
plt.legend(["True", "Prediction"])
ax.set(xlabel = "Sample no.", ylabel = "Value")

#MSE calculation on Test dataset
mse_test = mean_squared_error(y_test,pred)
print(mse_test)

#Assigning column number for all 15 features
x = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15])

#Learned coef after training the model
learned_coef= model.coef_

#Plot learned vs true coeff
fig, ax = plt.subplots()
ax = sns.lineplot(x,coef)
ax_1 = sns.lineplot(x,learned_coef)
plt.legend(labels =[ "true coeff", "learned coeff"])
ax.set(xlabel = "Feature", ylabel = "Coeff Value")

"""# Linear Regression with Top 2 coefficients"""

coef

import statsmodels.api as s

X1 = s.add_constant(X_train)
estimator = s.OLS(y_train, X1).fit()
print(estimator.summary())

#Considering the top 2 coefficients i.e., the coeff with lowest p values
X_train_new_0 = X_train[:,0].reshape(-1,1)
X_train_new_1 = X_train[:,2].reshape(-1,1)
X_test_new_0 = X_test[:,0].reshape(-1,1)
X_test_new_1 = X_test[:,2].reshape(-1,1)

#Training and prediction using the top 2 coefficients
model_new_0 = LinearRegression()
model_new_0.fit(X_train_new_0, y_train)
predictions_0 = model_new_0.predict(X_train_new_0)
predictions_test_0 = model_new_0.predict(X_test_new_0)

model_new_1 = LinearRegression()
model_new_1.fit(X_train_new_1, y_train)
predictions_1 = model_new_0.predict(X_train_new_1)
predictions_test_1 = model_new_0.predict(X_test_new_1)

#MSE for the new model
mse_new_0 = mean_squared_error(y_train, predictions_0)
mse_new_1 = mean_squared_error(y_train, predictions_1)
mse_new_test_0 = mean_squared_error(y_test, predictions_test_0)
mse_new_test_1 = mean_squared_error(y_test, predictions_test_1)
print(mse_new_0, mse_new_1)
print(mse_new_test_0, mse_new_test_1)

"""**Regression Line fit for the top 2 coefficients**"""

plt.scatter(X_train_new_0, y_train, color="red")
plt.plot(X_train_new_0, predictions_0)

plt.scatter(X_train_new_1, y_train, color="orange")
plt.plot(X_train_new_1, predictions_1)

"""# Observations

1. The MSE on train and test set with all features are 9.47 and 9.15 respectively. This indicates that the model is not overfit to the training data. The model trained has been generalized to samples given. 

2. The learning achieved by the coefficients after training stands proof to the fact that the model has indeed been generalized. Comparing the true and learned coefficients it's clear that the final values of these coefficients have been based on the impact they would have on the final output.

3. In selecting the top 2 coefficients, we have considered the ones with lowest p values and performed a linear regression on that data.

4. By doing so, we observe that the MSE of the test data is less than train data for the first feature and a bit higher for the next, but significantly lesser than the model with all features included. This indicates that a high variance estimate.

6. In the regression line fit graph, it can be inferred that the line fit may not give the best results as it does not cover the best(maximum) possible points in the features


"""

